<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8">
  <title>Notes on Linear Regression Methods</title>
  <meta name="description" content="notes on linear regression methods">
  <meta name="author" content="Matteo Del Seppia">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/post.css">

  <link rel="icon" type="image/png" href="../images/notes.png">

</head>
<body>

  <div class="container">
    <header style="margin: auto; text-align: center; margin-top: 30px;"><h3>Notes on Linear Regression Methods</h3></header>
    <p class="post-description">
        Least squares, ridge regression and lasso regression formulations, approached via both statistics and convex optimization.
        Partly based on Chapter 3 of <a href="https://hastie.su.domains/ElemStatLearn/"><i>Elements of Statistical Learning</i></a>.
    </p>

    <h4>
        1. Least Squares
    </h4>

    <h5>1.1 Least Squares for Linear Regression in Statistical Learning</h5>

    <p>
        We have an input vector <b>X</b> of <i>p</i>-dimensions. Our goal is to predict a real-valued output <b>Y</b>. Our linear regression
        model has the form:
        $$ f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j $$

        We can see that we search for a model that assigns a <i>weight</i> to each feature to predict the output. 
    </p>

    <p>
        Typically we have a set of <b>training</b> data <i>( x<sub>i</sub>, y<sub>i</sub> )</i>  for <i>i = 1...N</i>, from which we want
        to estimate the true parameters of our linear model. The most popular method to do this is the <i>least squares</i> method, where
        the coefficients &beta; minimize the residual sum of squares: 
        $$ RSS(\beta) = \sum_{i=1}^N (y_i - f(x_i))^2 $$
        $$ = \sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2$$
    </p>

    <p>
        To minimize <b>RSS</b>, we can express it as:
        $$ RSS(\beta) = (y - X \beta)^T(y - X \beta) , \ \ \
         X = \begin{bmatrix} 1 & x_{11} & ... & x_{1p} \\  
                             1 & x_{21} & ... & x_{2p} \\
                             . & . & ... & . \\
                             1 & x_{N1} & ... & x_{Np}
         \end{bmatrix} = \text{training set matrix}$$

         We can see that <b>RSS</b> is a quadratic function. Its gradient and Hessian with respect to &beta; are:
         $$ \nabla RSS  = -2X^T(y - X \beta) $$
         $$ \nabla^2 RSS = 2XX^T $$
    </p>

    <p>
        From convex optimization theory we know that if <b>XX<sup>T</sup></b> is postive definite then <b>RSS</b> is strongly convex
        and has a unique global optimum zeroing the gradient:
        $$ X^T(y - X \beta) = 0 \implies \hat{\beta} = (X^T X)^{-1} X^T y $$

        Remember that we are assuming that <b>X</b> is nonsingular, but in practice this is not a restrictive assumption.
    </p>

    <p>Now that we have our estimate of the parameters &beta; we can estimate the true signal:
        $$ \hat{y} = X \hat{\beta} = X (X^T X)^{-1} X^T y = Hy \ \ \text{(H is known as hat matrix)}$$

        Geometrically, the hat matrix computes the orthogonal projection of the training vector of outputs <b>y</b> on the subspace of <b>R<sup>N</sup></b> spanned
        by the training inputs <b>X</b>. This is why <b>H</b> is also known as <i>projection matrix</i>.
    </p>

    <p>
        What if the training set matrix is singular? In that case the solution of the unconstrained minimization problem is not uniquely
        defined, but the fitted values are still the projection of <b>y</b> onto the column space of <b>X</b>; there is just more than 
        one way to express that projection in terms of the column vectors of <b>X</b>.
        A common way to solve this is dropping the redundant columns in <b>X</b>.
    </p>

    <p>
        To study the sampling properties of our estimate, we now assume that the training outputs are uncorrelated and have constant 
        variance &sigma;<sup>2</sup>, and that the <i>x<sub>i</sub></i> are fixed (non-random). The covariance matrix of the least squares
        can be derived in this way:

        $$ \hat{\beta} = (X^T X)^{-1} X^T y $$
        $$  = (X^T X)^{-1} X^T (X \beta + \epsilon) $$
        $$ = \beta + (X^T X)^{-1} X^T \epsilon $$ 
        $$ \text{with $E(\epsilon) = 0$ and $ V(\epsilon) = \sigma^2 I $} $$

        $$ \ $$
        $$ V(\hat{\beta}) = V((X^T X)^{-1} X^T \epsilon) $$
        
        $$ = ((X^T X)^{-1} X^T) \  V(\epsilon) \ ((X^T X)^{-1} X^T)^T  $$
        $$ = \sigma^2 \ ((X^T X)^{-1} X^T) \ I \  ((X^T X)^{-1} X^T)^T $$
        $$ = \sigma^2 \ (X^T X)^{-1}(X^T X)(X^T X)^{-1} $$
        $$ = \sigma^2 \ (X^T X)^{-1} $$ 

        with &sigma;<sup>2</sup> typically estimated with:
        $$ \hat{\sigma}^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y_i})^2 \ ,$$
        $$ E(\hat{\sigma}^2) = \sigma^2 \ \ \ \text{(the estimate is unbiased)} $$
    </p>

    <br>
    <p>
        To make additional inferences about the parameters we have to add two assumptions:
        <ul>
            <li>
                the conditional expectation of <b>Y</b> is linear in <b>X<sub>1</sub>, ... , X<sub>p</sub></b>, meaning that we are certain
                that the true signal is a linear function given the features of the input data;
            </li>

            <li>
                the deviations of <b>Y</b> around its expectation are additive and Gaussian: $$ Y = \beta_0 + \sum_{j=1}^p X_j \beta_j + \epsilon $$ 
            </li>
        </ul>

        This assumptions are needed to make our final statement on the estimate of the true signal parameters:
        $$ \hat{\beta} \sim N(\beta, (X^T X)^{-1} \sigma^2 ) $$

        which is a very important statement, because it tells us that our model will be unbiased.
        </p>
        <p>
        It is also worth noting that:

        $$ (N-p-1) \hat{\sigma}^2 \sim \sigma^2 \chi_{N-p-1}^2 $$

        which is a chi-squared distribution with <i>N-p-1</i> degrees of freedom. 
        </p>
        <p>
            Intuitively, when <i>N-p</i> is high (the dataset is large with
        respect to the number of features) the variance of our estimation of the &beta;s is small and our regressor is efficient, meaning that
        its peformance on a test set will have a low mean squarred error.
        On the contrary, having a small dataset with respect to the number of features will make our model inefficient (with high variance) 
        when predicting the output of new unseen data.
        </p>        
    </p>

    <br>

    <h5>1.2 Gauss-Markov theorem</h5>
    <p>
        The theorem asserts that, under all the assumptions explicited above, the least squares estimator of the parameters of a linear regression model is the
        best linear unbiased estimator (BLUE), which means it has the smallest variance among all other unbiased estimators.

        To prove this, we can pick another linear, unbiased estimator:
        $$ \bar{\beta} = Cy, \ \ y = \beta X + \epsilon $$

        with <b>C</b> a row vector. Again, the estimator must be unbiased by construction:
        $$ E(\bar{\beta}) = C E(y) = C X \beta = \beta \ \  < = > \ \ CX = I$$

        Then:
        $$ V(\bar{\beta}) = V(Cy)  $$
        $$ = C V(y) C^T  $$
        $$ = \sigma^2 C C^T $$
        $$ \geq \sigma^2 C H C^T \ \ \ \text{(H is the projection matrix of the least squares estimate)}$$
        $$ = \sigma^2 C X (X^T X)^{-1} X^T C^T $$
        $$ = \sigma^2 (X^T X)^{-1} $$
        $$ = V(\hat{\beta}) $$

        The inequality holds thanks to the triangular inequality, as <b>H</b> is the orthogonal projection on the subspace spanned by <b>X</b>.
        <br>
        In fact, let <b>Q = I - H</b> be the projection onto the orthogonal complement of <b>X</b>. Then we have:
        $$ C^T = H C^T + Q C^T \ , \ H C^T \perp Q C^T $$
        $$ ||C||^2 = ||HC^T||^2 + ||QC^T||^2 $$
        $$ ||C||^2 \geq ||HC^T||^2 = (HC^T)^T(HC^T) = (CH)(HC^T) = CHC^T $$ 
        as any orthogonal projection matrix is idempotent (H^2 = H) and symmetric.<br>
        <br>
        While this is a strong result, note that there are <b>biased</b> methods like
        ridge regression that can still have a smaller variance than least squares. 

        </p>

        <p>
            In any case, least squares estimates in the real world might not
            satisfying enough for two reasons:
            <ul>
                <li>least squares often have low bias but large variance; prediction accuracy can sometimes
                    be improved by <i>shrinking</i> or setting some coefficients to zero (<i>subset selection</i>),
                    sacrificing a bit of bias to reduce the variance of the predicted values;
                 </li>

                 <li>
                    with a large number of variables, we would like to determine a smaller subset exhibiting
                    the stronger effects on the response vector <b>y</b>
                 </li>
            </ul>
        </p>

    <h5>1.3 Least Squares in Convex Optimization</h5>
    <p>
        Given a dataset of pairs <b>(x<sup>i</sup>, y<sup>i</sup>)</b> with <b>x<sup>i</sup></b> in <b>R<sup>N</sup></b>, <b>y<sup>i</sup></b> in <b>R</b> and <i>i = 1 ... p</i>, we are searching
        for a fitting hyperplane in the form:
        $$ f(x) = w^T x + b \ \ , \ w \in \mathbb{R}^N, \ b \in \mathbb{R}$$

        In particular, we want to find the hyperplane that minimizes the least squares cost function:
        $$ g(w, b) = \sum_{j = 1}^p (w^T x^j + b - y^j)^2 $$

        The problem can be expressed in a more convenient way
        by defining $$ z = (b, w), \ \  X = \begin{bmatrix} 1 & x_1^1 & ... & x_N^1 \\ . & . & ... & . \\ 1 & x_1^p & ... & x_p^N \end{bmatrix} \ \ ,
        $$
        
        $$
            \begin{cases}
            \min_{z} \frac{1}{2}||Xz - y||_2^2 \\
            \\
            z \in \mathbb{R}^{N+1}
            \end{cases}

            \equiv 

            \begin{cases}
            \min_{z} \frac{1}{2}z^T X^T X z - y^T X z + y^T y \\
            \\
            z \in \mathbb{R}^{N+1}
            \end{cases}
        $$

        which is an unconstrained quadratic convex problem. Note that the problem has always at least one solution even if X is nonsingular
        because <b>X <sup>T</sup>X</b> is positive semidefinite. All the solutions can be found by zeroing the gradient and solving
        the system of linear equations:
        $$ \nabla_z ( \frac{1}{2}z^T X^T X z - y^T X z + y^T y) = 0 \ \ , $$
        $$ X^T X z - X^T y = 0 \ \ , $$
        $$ X^T X z = X^T y $$

        which is exactly the solution we found when we approached the problem via statistical learning. <br>

    </p>

    <br><br>

    <h4>2. Shrinkage methods</h4>

    Subset selection produces a model that is interpretable and with possibly a lower prediction
    error than the full least squares model. But subset selection is a <i>discrete</i> process: either
    a feature is dropped or it is retained, and this often leads to high variance again. <br>
    On the other hand, shrinkage methods are more continuous, as they <i>shrink</i> or <i>increase</i> the importance of features without dropping
    them if not necessary. They are popular because they don't suffer as much of high variability. 

    <h5>2.1 Ridge Regression Model</h5>
    <p>
        Ridge regression shrinks the coefficients by adding a penalty to their squared size in the residual sum of squares cost function:
        $$ RSS^{\text{ridge}}(\beta) = \sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2 + \lambda \sum_{j=1}^p \beta_j^2 $$

        where <b>&lambda;</b> &geq; 0 is a given parameter controlling the amount of shrinking. When the shrinking parameter is zero, we have the standard
        least squares method, while when &lambda; &RightArrow; &infin; we have a flat hyperplane because all the &beta;s are zero but &beta;<sub>0</sub>.
    </p>

    <p>Moreover, it can be proven that minimizing the unconstrained residual sum of squares given &lambda; > 0 is equivalent to solving the constrained
        optimization problem:
        $$ \begin{cases}
                \min_\beta \sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2 \\
                \\
                \sum_{j=1}^p \beta_j^2 \leq t
            \end{cases}
        $$

        where <i><b>t</b></i> has a one-to-one correspondence with the given &lambda; :
        $$ t = y^T X (X^T X - \lambda I)^{-2} X^T y $$

        Notice that the intercept has been left out of the penalty. The penalization
        of the intercept would make the procedure depend on the origin chosen for 
        <b>Y</b>. Instead, we estimate the intercept as the mean of the given response
        vector. The remaining coefficients are estimated by a ridge regression without
        intercept, using the training data <b>X</b> after standardization and removing
        the column relative to the intercept.
    </p>

    <p>
        Writing the criterion in matrix form, we have:
        $$ RSS(\lambda) = (y - X \beta)^T (y - X \beta) + \lambda \beta^T \beta $$
        $$ \hat{\beta}^{\text{ridge}} = (X^T X + \lambda I )^{-1} X^T y $$

        In this case the solution adds a positive constant &lambda; to the diagonal of
        <b>X<sup>T</sup>X</b> before inversion. This makes the problem <b>always nonsingular</b>.
    </p>

    <p>An interesting interpretation of the ridge regression method can be achieved
        looking at the <i>singular value decomposition</i> of the centered input matrix
        <b>X</b>. The SVD of the <i>N x p</i> matrix <b>X</b> is:
        $$ X = UDV^T, \ \ U \in \mathbb{R}^{N \times p}, V \in \mathbb{R}^{p \times p} $$
        such that:
        <ul>
            <li>
                the columns of <b>U</b> span the column space of <b>X</b>
            </li>
            <li>
                the columns of <b>V</b> span the row space of <b>X</b>
            </li>
            <li>
                <b>D</b> is <i>p x p</i> diagonal matrix, with diagonal entries
                <i>d<sub>1</sub> &geq; ... &geq; d<sub>p</sub></i> known as <b>singular
                    values
                </b> of <b>X</b>. If one or more values <i>d<sub>j</sub></i> = 0, <b>X</b>
                is singular
            </li>
        </ul>

        Let's apply the singular value decomposition to the ridge regression solutions:
        $$ X \hat{\beta}^{\text{ridge}} = X(X^T X + \lambda I)^{-1} X^T y$$
        $$ = U D(D^2 + \lambda I)^{-1} D U^T y $$
        $$ = \sum_{j = 1}^p \frac{d_j^2}{d_j^2 + \lambda} u_j^T y $$

        where <i>u<sub>j</sub></i> are the columns of <b>U</b>. Also notice that:
        $$ \frac{d_j^2}{d_j^2 + \lambda} \leq 1 \ \ \ \forall j=1...p $$
        because the shrinking factor is non-negative. 
    </p>

    <p>
        It is now clear that ridge regression computes the coordinates of <b>y</b> with
        respect to the orthonormal basis <b>U</b> and then shrinks the coordinates by the
        factors <i>d<sub>j</sub> <sup>2</sup> / ( d<sub>j</sub> <sup>2</sup> + &lambda; )</i>, which
        means that a greater amount of shrinkage is applied to the coordinates of basis vectors
        with smaller <i>d<sub>j</sub> <sup>2</sup></i>.<br>
        
        The SVD of the centered matrix <b>X</b> is just another way of expressing
        the principal components of the variables in <b>X</b>. In fact, the sample
        covariance matrix is given by <b>X <sup>T</sup> X</b> <i>/N</i>, and with the 
        decomposition we get:
        $$ X^T X = V D^2 V^T $$

        which is the eigen decomposition of the covariance matrix up to a factor <i>N</i>.
        The columns of <b>V</b> are the eigenvectors of <b>X</b> and are also known as
        the <i>principal components directions</i> of <b>X</b>.
        The first principal component direction <i>v <sub>1</sub></i> has the property that
        <b>X</b><i>v<sub>1</sub></i> has the largest sample variance amongst all normalized
        linear combinations of the columns of <b>X</b>:
        $$ Var(X v_1) = \frac{d_1^2}{N} $$
        $$ \text{and} $$
        $$ z_1 = Xv_1 = u_1 d_1 $$

        where <i>z<sub>1</sub></i> is the first <i>princiapl component</i> of <b>X</b> and
        <i>u<sub>1</sub></i> is the first <i>normalized principal component</i> of <b>X</b>.
    </p>

    <p>
        The last principal components have minimum variance, and ridge regression shrinks
        these directions coefficients the most.
        The implicit assumption of ridge regression is that the response will tend to vary most in the
        directions of high variance of the inputs, which is often a reasonable assumption.
    </p>

    <p>Lastly, we define the following monotone decreasing function of &lambda; the 
        <i>effective degrees of freedom</i> of the ridge regression fit:
        $$ df(\lambda) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda} $$
    </p>

    <h5>2.2 Lasso Regression Model</h5>
    <p>
        Lasso is a shrinkage method like ridge, but the residual sum of squares
        here is defined as:

        $$ RSS^{\text{lasso}}(\beta) = \sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2 + \lambda \sum_{j=1}^p | \beta_j | $$

        and the equivalent minimization problem is:

        $$ \begin{cases}
        \min_\beta \frac{1}{2}\sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2 \\
        \\
        \sum_{j=1}^p | \beta_j | \leq t
        \end{cases}
        $$

        Just like in ridge regression, we can re-parametrize the intercept by standardizing the
        predictors and the solution for the intercept is the mean of the response vector.
        Computing the lasso regression is a quadratic programming problem with the same
        computational cost of ridge ression.
    </p>
    <p>
        In this case the constraint is non-smooth and setting &lambda; sufficiently high
        will have the effect of putting some coefficients to exactly zero.

        Moreover, both ridge and lasso regression find the first point where the
        level sets of the residual sum of squares hit the constraint region.
        
        In lasso regression the constraint region is a rhomboid and when <i>p > 2</i>
        it has many corners, flat edges and faces; there are many possibilities that
        for one of the estimated parameters to be set to zero.

    <h5>2.3 Elatic Net Regression</h5>
    <p>
        Elastic net regression takes the best of the two methods. In fact, its penalty
        term is designed to relax the rhomboid's constraints, making its faces smooth similarly to what happens in ridge regression,
        while maintaining the corners sharp (non-differentiable)
        to be still able to put some coefficients to zero:

        $$ RSS^{\text{elastic}} = \sum_{i=1}^N \big( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \big)^2 + \lambda \sum_{j=1}^p (\alpha\beta_j^2 + (1-\alpha)|\beta_j|) \ , \ \alpha \in [0, 1] $$
    </p>

    <h5>2.4 Principal Components Regression</h5>
    <p>
        Principal component regression takes the transformed input columns <b>z<sub>i</sub> = Xv<sub>i</sub></b> of the centered
        training data <b>X</b> and then regresses <b>y</b> on the principal components for <i>i = 1... M < p</i>.
        Since the principal components are orthogonal, the regression is just a sum of univariate regressions:

        $$ \hat{y}_{(M)}^{\text{pcr}} = \bar{y}\textbf{1} + \sum_{m=1}^M \hat{\theta}_m z_m $$

        $$ \hat{\beta}^{\text{pcr}}(M) = \sum_{m=1}^M \hat{\theta}_m v_m $$

        $$ \hat{\theta}_m = \frac{z_m^T y}{||z_m||^2} $$
    </p>

    <p>
        Principal components regression is similar to ridge regression as both
        operate via the principal components of the input matrix. Ridge shrinks 
        a certain coefficient more depending on the size of the corresponding eigenvalue,
        while <b>PCR discards the <i>p-M</i> smallest eigenvalue components.</b>
    </p>
    <br>
    <br>
</div>
</body>
</html>
